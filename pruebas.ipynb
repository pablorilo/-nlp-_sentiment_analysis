{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMUC4166HODNFtoYW79p9XN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"35wYNy3XFsZY"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import codecs\n","import os\n","import pandas as pd\n","import numpy as np\n","import string\n","import random\n","import pickle\n","\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.probability import FreqDist\n","\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","from sklearn.model_selection import train_test_split,GridSearchCV # Modelado\n","from sklearn.pipeline import Pipeline # Modelado\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer # Modelado\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import chi2\n","from sklearn.linear_model import LogisticRegression # Reporte\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, precision_recall_curve\n","\n","path = '/content/drive/MyDrive/NLP_practica/datasets/' #path en donde esta almacenado el df"],"metadata":{"id":"DnclTQX2hgGX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TextPipelineLogistic():\n","  \"\"\"Esta clase genera el pipeline indicado realizando la extracción de características, calcula los mejores hyperparametros, entrena el modelo con dichos parametros y realiza las predicciones.\n","  A lo largo del proceso se iran mostrando en pantalla información del paso que se está realizando asi como algun dato de interes\"\"\"\n","\n","  def __init__(self, method_extraction:str = 'tfidf', model:str= 'regresion') -> None:\n","    \"\"\"el constructor de la clase crea las variables de clase que contienen los hyperparámetros a validar. Por defecto se ejecutan diferentes rangos de los hyperparámtros, si se quiere modificar el rango, \n","    se puede pasar como argumento al crear la instancia\"\"\"\n","    self.method_extraction = method_extraction\n","    self.model = model\n","    self.params = self.createDictHyperparams()\n","    self.pipe = self.pipelineMethod()\n","    \n","  def run(self,X_train: pd.Series, X_test: pd.Series, y_train: pd.Series, y_test: pd.Series) -> tuple:\n","    \"\"\"este método ejecuta todo el pipeline indicado automaticamente, y va generando mensajes durante todo el proceso\n","      : param X_train: pd.Series  conjunto de train\n","      : param X_test: pd.Series  conjunto de test\n","      : param y_train: pd.Series  etiquetas de train\n","      : param y_test: pd.Series  etiquetas de test\n","      : param method_extraction:str = 'tfidf' parámetro que define el pipeline,  por defecto TfidfVectorizer. se puede pasar por parámetro 'countVectorizer\n","      return : tuple --model.best_params_,train_predict, test_predict\"\"\"\n","    \n","    pipeline_steps = list(self.pipe.named_steps.keys())\n","    print(f'Iniciamos el cross-validation del pipeline: {pipeline_steps[0]} ->  {pipeline_steps[1]} -> {pipeline_steps[2]}.\\n')\n","    print('Hiperparámetros a validar en la cross-validation')\n","    self.printDict(self.params)\n","    self.best_model = self.gridSearch(pipeline = self.pipe, params = self.params)\n","    model = self.fitGridModel(best_model = self.best_model, X_train= X_train, y_train= y_train)\n","    print('\\nObtenemos los valores optimos para los hiperparámtros')\n","    print('<<<<<<<<<<<<<<<<<Best Hyperparamns>>>>>>>>>>>>>>>>>>>')\n","    self.printDict(model.best_params_)\n","    train_predict, test_predict = self.predictModel(model,X_train= X_train,X_test= X_test)\n","    print('\\nMétricas obtenidas en el conjunto de train')\n","    self.evaluateModel(y_train,train_predict)\n","    print('\\nMétricas obtenidas en el conjunto de test')\n","    self.evaluateModel(y_test,test_predict)\n","    return model.best_params_,train_predict, test_predict\n","\n","    \n","  def createDictHyperparams(self,tfidf__ngram_range: list= [(1, 1), (1, 2)], tfidf__min_df: list = [1, 2],\n","               tfidf__max_df:list = [0.9, 0.95],tfidf__max_features:list = [1000,2500], \n","               chi2__k:list = [10, 100], logistic__C:list = [0.1, 1.0],\n","               n_estimators:list = [50, 100], max_depth:list= [3, 5], learning_rate:list = [0.01,0.1]):\n","    if self.method_extraction == 'tfidf':\n","      param_extract = {\n","      'tfidf__ngram_range':  tfidf__ngram_range,\n","      'tfidf__min_df': tfidf__min_df,\n","      'tfidf__max_df': tfidf__max_df,\n","      'tfidf__max_features': tfidf__max_features,\n","      'chi2__k': chi2__k,      \n","                        }\n","    if self.method_extraction == 'countVectorizer':\n","      param_extract = {\n","      'chi2__k': chi2__k\n","                     }\n","\n","    if self.model == 'regresion':\n","      param_model = {\n","      'logistic__C': logistic__C\n","                      }\n","    if self.model == 'boosting':\n","      param_model = {\n","      'n_estimators': n_estimators,\n","      'max_depth': max_depth,\n","      'learning_rate': learning_rate\n","                          }\n","    return {**param_extract,**param_model}\n","\n","\n","  def pipelineMethod(self) -> Pipeline:\n","    \"\"\"método que carga el pipeline dependiendo del método de extraccion\n","      : param method_extraction: puede ser 'tfidf' o 'countVectorizer'\n","      return : Pipeline\"\"\"\n","    print('[INFO] Generando el pipeline...')\n","    if self.method_extraction == 'tfidf':\n","      if self.model =='regresion':\n","        pipeline = Pipeline([\n","            ('tfidf', TfidfVectorizer(ngram_range=(1, 1), min_df=1, max_df=1.0, max_features = 2500)),\n","            ('chi2', SelectKBest(chi2)),\n","            ('logistic', LogisticRegression(max_iter=1000))\n","        ])\n","      if self.model =='boosting':\n","        pipeline = Pipeline([\n","            ('tfidf', TfidfVectorizer(ngram_range=(1, 1), min_df=1, max_df=1.0, max_features = 2500)),\n","            ('chi2', SelectKBest(chi2)),\n","            ('boosting', GradientBoostingClassifier())\n","        ])\n","      return pipeline\n","    if self.method_extraction == 'countVectorizer':\n","      if self.model =='regresion':\n","        pipeline = Pipeline([\n","            ('countvectorizer',CountVectorizer()),\n","            ('chi2', SelectKBest(chi2)),\n","            ('logistic', LogisticRegression())\n","          ]) \n","      if self.model =='boosting':\n","        pipeline = Pipeline([\n","            ('countvectorizer',CountVectorizer()),\n","            ('chi2', SelectKBest(chi2)),\n","            ('boosting', GradientBoostingClassifier())\n","          ]) \n","        \n","    return pipeline\n","    \n","\n","  def gridSearch(self, pipeline: Pipeline, params: dict, cv: int = 5) -> GridSearchCV:\n","    \"\"\"metodo que carga el GridSearchCV\n","     : param pipeline: Pipeline \n","     : param params: dict contien los hyperparámetros y los rangos a validar\n","     : param cv: int por defecto 5. número de validaciones\n","     return : objeto de la clase GridSearchCV\"\"\"\n","    print('\\n[INFO] Realizando el gridSeach ...')\n","    grid_search = GridSearchCV(pipeline, params, cv=cv, n_jobs=-1, verbose=1)\n","    return grid_search\n","\n","  def fitGridModel(self, best_model: GridSearchCV, X_train: pd.Series, y_train: pd.Series) -> GridSearchCV:\n","    \"\"\"con este metedo se realiza el entrenamiento en base al modelo GridSearchCV pasado por parámetro\n","      : param best_model: GridSearchCV     \n","      : param X_train: pd.Series\n","      : param X_train: pd.Series\n","      return GridSearchCV entrenado \"\"\"\n","    best_model.fit(X_train, y_train)\n","    return best_model\n","\n","  def predictModel(self, model, X_train: pd.Series,X_test: pd.Series) -> tuple:\n","    \"\"\"con este metedo se realiza la prediccion del conjunto de train t test\n","      : param model: GridSearchCV     \n","      : param X_train: pd.Series\n","      : param X_train: pd.Series\n","      return tuple (train_predict, test_predict)\"\"\"\n","    print('\\n[INFO] Realizando las predicciones del conjunto de train ...')\n","    train_predict = model.predict(X_train)\n","    print('[INFO] Realizando las predicciones del conjunto de test ...')\n","    test_predict = model.predict(X_test)\n","    return train_predict, test_predict\n","\n","  @staticmethod\n","  def evaluateModel(y_true: pd.Series, y_pred: pd.Series) -> None:\n","    \"\"\"método que calcula e imprime por pantalla diferentes métricas\n","      : param y_true: etiquetas\n","      : param y_pred: predicciones\n","      return: None\"\"\"\n","    acc = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","    cm = confusion_matrix(y_true, y_pred)\n","    \n","    print('Accuracy: {:.4f}'.format(acc))\n","    print('Precision: {:.4f}'.format(precision))\n","    print('Recall: {:.4f}'.format(recall))\n","    print('F1-Score: {:.4f}'.format(f1))\n","    print('Confusion Matrix:\\n{}'.format(cm))\n","\n","  def printDict(self,dic: dict) -> None:\n","    \"\"\"metodo al que le pasas un diccionario e itera en los items para imprimirlos '{key}:    {value}'\n","     : param dic: dict\n","     return: None\"\"\"\n","    for key, value in dic.items():\n","      print(f'{key}:    {value}')\n","\n","\n"],"metadata":{"id":"x7wSbp92pn8X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ov7IfZerx5oy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TextPipelineLogistic():\n","  \"\"\"Esta clase genera el pipeline indicado realizando la extracción de características, calcula los mejores hyperparametros, entrena el modelo con dichos parametros y realiza las predicciones.\n","  A lo largo del proceso se iran mostrando en pantalla información del paso que se está realizando asi como algun dato de interes\"\"\"\n","\n","  def __init__(self,tfidf__ngram_range: list= [(1, 1), (1, 2)], tfidf__min_df: list = [1, 2], tfidf__max_df:list = [0.9, 0.95],tfidf__max_features:list = [1000,2500], chi2__k:list = [10, 100], logistic__C:list = [0.1, 1.0]) -> None:\n","    \"\"\"el constructor de la clase crea las variables de clase que contienen los hyperparámetros a validar. Por defecto se ejecutan diferentes rangos de los hyperparámtros, si se quiere modificar el rango, \n","    se puede pasar como argumento al crear la instancia\"\"\"\n","    self.params_tfidf = {\n","      'tfidf__ngram_range':  tfidf__ngram_range,\n","      'tfidf__min_df': tfidf__min_df,\n","      'tfidf__max_df': tfidf__max_df,\n","      'tfidf__max_features': tfidf__max_features,\n","      'chi2__k': chi2__k,\n","      'logistic__C': logistic__C\n","    }\n","    self.params_counV = {\n","      'chi2__k': chi2__k,\n","      'logistic__C': logistic__C\n","    }\n","\n","  def run(self,X_train: pd.Series, X_test: pd.Series, y_train: pd.Series, y_test: pd.Series, method_extraction:str = 'tfidf') -> tuple:\n","    \"\"\"este método ejecuta todo el pipeline indicado automaticamente, y va generando mensajes durante todo el proceso\n","      : param X_train: pd.Series  conjunto de train\n","      : param X_test: pd.Series  conjunto de test\n","      : param y_train: pd.Series  etiquetas de train\n","      : param y_test: pd.Series  etiquetas de test\n","      : param method_extraction:str = 'tfidf' parámetro que define el pipeline,  por defecto TfidfVectorizer. se puede pasar por parámetro 'countVectorizer\n","      return : tuple --model.best_params_,train_predict, test_predict\"\"\"\n","    \n","    if method_extraction == 'tfidf': \n","      print('Iniciamos el cross-validation del pipeline: TfidfVectorizer -> SelectKBest(chi2) -> LogisticRegression\\n')\n","      print('Hiperparámetros a validar en la cross-validation')\n","      self.printDict(self.params_tfidf)\n","      self.pipe = self.pipelineMethod(method_extraction)      \n","      self.best_model = self.gridSearch(pipeline = self.pipe, params = self.params_tfidf)\n","      model = self.fitGridModel(best_model = self.best_model, X_train= X_train, y_train= y_train)\n","      print('\\nObtenemos los valores optimos para los hiperparámtros')\n","      print('<<<<<<<<<<<<<<<<<Best Hyperparamns>>>>>>>>>>>>>>>>>>>')\n","      self.printDict(model.best_params_)\n","      train_predict, test_predict = self.predictModel(model,X_train= X_train,X_test= X_test)\n","      print('\\nMétricas obtenidas en el conjunto de train')\n","      self.evaluateModel(y_train,train_predict)\n","      print('\\nMétricas obtenidas en el conjunto de test')\n","      self.evaluateModel(y_test,test_predict)\n","      return model.best_params_,train_predict, test_predict\n","\n","    elif method_extraction == 'countVectorizer':      \n","      print('Iniciamos el cross-validation del pipeline: CountVectorizer -> SelectKBest(chi2) -> LogisticRegression\\n')\n","      self.pipe = self.pipelineMethod(method_extraction)\n","      print('Hiperparámetros a validar en la cross-validation')\n","      self.printDict(self.params_counV)\n","      self.best_model = self.gridSearch(pipeline = self.pipe, params = self.params_counV)\n","      model = self.fitGridModel(best_model = self.best_model, X_train= X_train, y_train= y_train)\n","      print('\\nObtenemos los valores optimos para los hiperparámtros')\n","      print('<<<<<<<<<<<<<<<<<Best Hyperparamns>>>>>>>>>>>>>>>>>>>')\n","      self.printDict(model.best_params_)\n","      train_predict, test_predict = self.predictModel(model,X_train= X_train,X_test= X_test)\n","      print('\\nMétricas obtenidas en el conjunto de train:')\n","      self.evaluateModel(y_train,train_predict)\n","      print('\\nMétricas obtenidas en el conjunto de test:')\n","      self.evaluateModel(y_test,test_predict)    \n","      return model.best_params_,train_predict, test_predict\n","\n","    else:\n","      raise TypeError ('El método indicado no se puede ejecutar')   \n","        \n","  def pipelineMethod(self, method_extraction:str) -> Pipeline:\n","    \"\"\"método que carga el pipeline dependiendo del método de extraccion\n","      : param method_extraction: puede ser 'tfidf' o 'countVectorizer'\n","      return : Pipeline\"\"\"\n","    print('[INFO] Generando el pipeline...')\n","    if method_extraction == 'tfidf':\n","      pipeline = Pipeline([\n","          ('tfidf', TfidfVectorizer(ngram_range=(1, 1), min_df=1, max_df=1.0, max_features = 2500)),\n","          ('chi2', SelectKBest(chi2)),\n","          ('logistic', LogisticRegression(max_iter=1000))\n","      ])\n","      return pipeline\n","    if method_extraction == 'countVectorizer':\n","      pipeline = Pipeline([\n","          ('countvectorizer',CountVectorizer()),\n","          ('chi2', SelectKBest(chi2)),\n","          ('logistic', LogisticRegression())\n","         ]) \n","      return pipeline\n","\n","  def gridSearch(self, pipeline: Pipeline, params: dict, cv: int = 5) -> GridSearchCV:\n","    \"\"\"metodo que carga el GridSearchCV\n","     : param pipeline: Pipeline \n","     : param params: dict contien los hyperparámetros y los rangos a validar\n","     : param cv: int por defecto 5. número de validaciones\n","     return : objeto de la clase GridSearchCV\"\"\"\n","    print('\\n[INFO] Realizando el gridSeach ...')\n","    grid_search = GridSearchCV(pipeline, params, cv=cv, n_jobs=-1, verbose=1)\n","    return grid_search\n","\n","  def fitGridModel(self, best_model: GridSearchCV, X_train: pd.Series, y_train: pd.Series) -> GridSearchCV:\n","    \"\"\"con este metedo se realiza el entrenamiento en base al modelo GridSearchCV pasado por parámetro\n","      : param best_model: GridSearchCV     \n","      : param X_train: pd.Series\n","      : param X_train: pd.Series\n","      return GridSearchCV entrenado \"\"\"\n","    best_model.fit(X_train, y_train)\n","    return best_model\n","\n","  def predictModel(self, model, X_train: pd.Series,X_test: pd.Series) -> tuple:\n","    \"\"\"con este metedo se realiza la prediccion del conjunto de train t test\n","      : param model: GridSearchCV     \n","      : param X_train: pd.Series\n","      : param X_train: pd.Series\n","      return tuple (train_predict, test_predict)\"\"\"\n","    print('\\n[INFO] Realizando las predicciones del conjunto de train ...')\n","    train_predict = model.predict(X_train)\n","    print('[INFO] Realizando las predicciones del conjunto de test ...')\n","    test_predict = model.predict(X_test)\n","    return train_predict, test_predict\n","\n","  @staticmethod\n","  def evaluateModel(y_true: pd.Series, y_pred: pd.Series) -> None:\n","    \"\"\"método que calcula e imprime por pantalla diferentes métricas\n","      : param y_true: etiquetas\n","      : param y_pred: predicciones\n","      return: None\"\"\"\n","    acc = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","    cm = confusion_matrix(y_true, y_pred)\n","    \n","    print('Accuracy: {:.4f}'.format(acc))\n","    print('Precision: {:.4f}'.format(precision))\n","    print('Recall: {:.4f}'.format(recall))\n","    print('F1-Score: {:.4f}'.format(f1))\n","    print('Confusion Matrix:\\n{}'.format(cm))\n","\n","  def printDict(self,dic: dict) -> None:\n","    \"\"\"metodo al que le pasas un diccionario e itera en los items para imprimirlos '{key}:    {value}'\n","     : param dic: dict\n","     return: None\"\"\"\n","    for key, value in dic.items():\n","      print(f'{key}:    {value}')\n","\n","\n"],"metadata":{"id":"j9v1k3qVx5z4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pipeline = Pipeline([\n","            ('countvectorizer', CountVectorizer()),\n","            ('chi2', SelectKBest(chi2)),\n","            ('boosting', GradientBoostingClassifier())\n","          ])\n"],"metadata":{"id":"_9qdDEtGfd-2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a = list(pipeline.named_steps.keys())"],"metadata":{"id":"2OsisqZxs23L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"_hHNlfY8OAIx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gensim\n","import multiprocessing as mp\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import (\n","    Dense,\n","    Dropout,\n","    Embedding,\n","    LSTM,\n",")\n","from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n","from sklearn.preprocessing import LabelEncoder"],"metadata":{"id":"deaUrPkcONU9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = '/content/drive/MyDrive/NLP_practica/datasets/' #\n","df = pd.read_csv(f'{path}df_balanced.csv')"],"metadata":{"id":"SiHtKUF9tCKt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Parámetros del WORD2VEC\n","W2V_SIZE = 300 # tamaño de vectores\n","W2V_WINDOW = 7 # número de palabras que va a mirar alrededor\n","# 32\n","W2V_EPOCH = 5 # número de epoca\n","W2V_MIN_COUNT = 2 #número mínimo de frecuencia\n","\n","# KERAS\n","SEQUENCE_LENGTH = 500 # número de secuencias de keras"],"metadata":{"id":"jo6B9XXlOQAE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PreprocessingCNN():\n","  \"\"\"pipeline de preprocesado que consta de: EliminacionNulos -> Normalización -> Lemmatización -> EliminarStopWord\"\"\"\n","  def __init__(self, vocab_size, max_len, embedding_matrix=None):\n","        self.tokenizer = Tokenizer(num_words=vocab_size)\n","        self.max_len = max_len\n","        self.embedding_matrix = embedding_matrix\n","\n","  def generateTokenizer(self, train_df):\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(train_df)\n","    vocab_size = len(tokenizer.word_index) + 1\n","    print(f\"Total words: {vocab_size}\")\n","    return tokenizer, vocab_size\n","\n","  def generateWord2vec(self, train_df):\n","    documents = [_text.split() for _text in train_df.review]\n","    w2v_model = gensim.models.word2vec.Word2Vec(\n","        size=W2V_SIZE,\n","        window=W2V_WINDOW,\n","        min_count=W2V_MIN_COUNT,\n","        workers=mp.cpu_count(),\n","    )\n","    w2v_model.build_vocab(documents)\n","\n","    words = w2v_model.wv.vocab.keys()\n","    vocab_size = len(words)\n","    print(f\"Vocab size: {vocab_size}\")\n","    w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)\n","\n","    return w2v_model\n","\n","  def generateEmbedding(self, word2vec_model, vocab_size, tokenizer):\n","    embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\n","    for word, i in tokenizer.word_index.items():\n","        if word in word2vec_model.wv:\n","            embedding_matrix[i] = word2vec_model.wv[word]\n","    return Embedding(\n","        vocab_size,\n","        W2V_SIZE,\n","        weights=[embedding_matrix],\n","        input_length=SEQUENCE_LENGTH, \n","        trainable=False,\n","    )\n","  @staticmethod\n","  def deleteNan(df: pd.DataFrame) -> pd.DataFrame:\n","    #Eliminamos los documentos que no tienen review\n","    mask = df['text_length'] != 0 #mascara boleana \n","    return df.loc[mask]\n","\n","  \n"],"metadata":{"id":"zbRe3YmfK4JQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = '/content/drive/MyDrive/NLP_practica/datasets/' #\n","df = pd.read_csv(f'{path}processed_df.csv')"],"metadata":{"id":"VbQkSes1-iyY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"Vdla8PfB_zau"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _build_embedding_matrix(self):\n","        embedding_dim = 100  # Dimensionality of the word embeddings\n","        word_index = self.tokenizer.word_index\n","        vocab_size = len(word_index) + 1  # Adding 1 because of reserved 0 index\n","        embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","        return embedding_matrix\n","\n","    def _create_embedding_layer(self):\n","      embedding_dim = 100  # Dimensionality of the word embeddings\n","      word_index = self.tokenizer.word_index\n","      vocab_size = len(word_index) + 1  # Adding 1 because of reserved 0 index\n","      embedding_matrix = self.embedding_matrix\n","      if embedding_matrix is None:\n","          embedding_matrix = self._build_embedding_matrix()\n","      embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=self.max_len,\n","                                  weights=[embedding_matrix], trainable=False)\n","      return embedding_layer"],"metadata":{"id":"Dk-WZF2MQMOz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","import multiprocessing as mp\n","\n","class TextPreprocessorCNN:\n","\n","    def __init__(self, df:pd.DataFrame,num_words:int = 1000):\n","      self.X_train, self.X_test, self.y_train, self.y_test = self.trainTestSplit(df)\n","      self.reviewMetrics()\n","      self.tokenizer_object = Tokenizer(num_words= num_words)\n","\n","    def preprocess(self, maxlen: int):  \n","      #tokenizer\n","      sequences_train, sequences_test, vocab_size = self.tokenizer()    \n","      # Padding\n","      padded_sequences_train = self.paddingSequences(sequences_train, maxlen)\n","      padded_sequences_test = self.paddingSequences(sequences_test, maxlen)  \n","      # #word2vec\n","      w2v_model = self.generate_word2vec(sequences_train)\n","      #embeding\n","      embedding_layer = self.generate_embedding(word2vec_model=w2v_model, vocab_size= vocab_size)\n","      return padded_sequences_train,padded_sequences_test ,embedding_layer,self.y_train, self.y_test\n","\n","    def trainTestSplit(self, df:pd.DataFrame, train_size: float=0.75, test_size: float=0.25, random_state:int =42, shuffle:bool =True):\n","      X_train, X_test, y_train, y_test = train_test_split(\n","      df['processed_tokens'],\n","      df['sentiment_label'],\n","      train_size=0.75,\n","      test_size=0.25,\n","      random_state=42,\n","      shuffle=True\n","       )\n","      return X_train, X_test, y_train, y_test\n","    def reviewMetrics(self):\n","      max_length = 0\n","      total_length = 0\n","      num_examples = len(self.X_train)\n","\n","      for example in self.X_train:\n","          length = len(example)\n","          total_length += length\n","          if length > max_length:\n","              max_length = length\n","      mean_length = total_length / num_examples\n","\n","      print('---------------------------------------')\n","      print(f'El tamaño máximo de review: {max_length}\\n')\n","      print('---------------------------------------')\n","      print(f'La media de palabras por review: {mean_length}\\n')\n","\n","    def tokenizer(self):\n","\n","      # Concatenar las columnas usando pd.concat()\n","      sequences = pd.concat([self.X_train, self.X_test],axis=0)\n","\n","      # Verificar que la concatenación se realizó correctamente\n","      self.tokenizer_object.fit_on_texts(sequences)\n","      vocab_size = len(self.tokenizer_object.word_index) + 1\n","      sequences_train = self.tokenizer_object.texts_to_sequences(self.X_train)\n","      sequences_test = self.tokenizer_object.texts_to_sequences(self.X_test)\n","      return sequences_train, sequences_test, vocab_size\n","\n","    def paddingSequences(self, sequences, maxlen: int):\n","      padded_sequences = pad_sequences(sequences, maxlen=maxlen)\n","      return padded_sequences\n","\n","    def generate_word2vec(self, sequences_train):     \n","      sequences_train = [str(sequence) for sequence in sequences_train]\n","   \n","      documents = [sequence.split() for sequence in sequences_train]\n","      w2v_model =Word2Vec(\n","          size=300,\n","          window=5,\n","          min_count=2,\n","          workers=mp.cpu_count(),\n","      )\n","      w2v_model.build_vocab(documents)\n","\n","      words = w2v_model.wv.vocab.keys()\n","      vocab_size = len(words)\n","      print(f\"Vocab size: {vocab_size}\")\n","      w2v_model.train(documents, total_examples=len(documents), epochs=2)\n","\n","      return w2v_model\n","\n","    def generate_embedding(self, word2vec_model, vocab_size):\n","      embedding_matrix = np.zeros((vocab_size, 300))\n","      for word, i in self.tokenizer_object.word_index.items():\n","          if word in word2vec_model.wv:\n","              embedding_matrix[i] = word2vec_model.wv[word]\n","      return Embedding(\n","          vocab_size,\n","          300,\n","          weights=[embedding_matrix],\n","          input_length=500, \n","          trainable=False,\n","    )\n","    \n","    \n","\n","  \n","\n","    \n"],"metadata":{"id":"jIgbu_74tfo1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.preprocessing.text import Tokenizer\n","from gensim.models import Word2Vec\n","from tensorflow.keras.layers import (\n","    Dense,\n","    Dropout,\n","    Embedding,\n","    LSTM,\n",")"],"metadata":{"id":"njUJ5B59OHMU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"G713XNTUhkuK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a = TextPreprocessorCNN(df)\n","padded_sequences_train,padded_sequences_test ,embedding_layer,y_train, y_test = a.preprocess(maxlen = 1200)"],"metadata":{"id":"HUwAD9W8Oteb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['padded_sequences'] = padded_sequences\n","df['embedding'] = embedding"],"metadata":{"id":"okBPEuUwrZpz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["padded_sequences.shape"],"metadata":{"id":"uYHryPgfmJPH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Embedding, LSTM, Dense, Dropout, GRUV2, SimpleRNN\n","from tqdm import tqdm\n","class TrainingCNN:\n","  def __init__(self,padded_sequences_train,padded_sequences_test, y_train, y_test ,embedding_layer):\n","    self.padded_sequences_train= padded_sequences_train\n","    self.padded_sequences_test= padded_sequences_test\n","    self.y_train = y_train\n","    self.y_test = y_test\n","    self.embedding_layer = embedding_layer\n","    \n","  def run(self, input_dim: int=10882, output_dim: int=100, input_length: int=1200,n_neural: int = 100):\n","    type_layers = ['LSTM','GRU','SimpleRNN']\n","    metrics_df = pd.DataFrame(columns=['layer', 'loss_train', 'accuracy_train', 'precision_train', 'recall_train', 'f1_train', 'loss_test', 'accuracy_test', 'precision_test', 'recall_test', 'f1_test'])\n","    for layer in type_layers:\n","      self.model_cnn_embedding = self.modelCnnEmbedding(rnn_type = layer, input_dim=input_dim, output_dim=output_dim, input_length=input_length,n_neural= n_neural)\n","      print(f'\\n[INFO]Entrenando modelo CNN con capa {layer}')\n","      self.model_cnn_embedding.fit(self.padded_sequences_train,self.y_train, validation_split=0.2,batch_size=64, epochs=1)\n","      print('\\nGeneramos las métricas de train:')\n","      metrics_train = self.evaluate_model(self.model_cnn_embedding,self.padded_sequences_train,self.y_train)\n","      print('\\nGeneramos las métricas de test:')\n","      metrics_test = self.evaluate_model(self.model_cnn_embedding,self.padded_sequences_test,self.y_test)\n","      metrics_dict = {'layer': layer,\n","                        'loss_train': metrics_train['loss'],\n","                        'accuracy_train': metrics_train['accuracy'],\n","                        'precision_train': metrics_train['precision'],\n","                        'recall_train': metrics_train['recall'],\n","                        'f1_train': metrics_train['f1'],\n","                        'loss_test': metrics_test['loss'],\n","                        'accuracy_test': metrics_test['accuracy'],\n","                        'precision_test': metrics_test['precision'],\n","                        'recall_test': metrics_test['recall'],\n","                        'f1_test':metrics_test['f1']\n","                       }\n","        \n","      metrics_df = metrics_df.append(metrics_dict, ignore_index=True)\n","    return metrics_df\n","  def modelCnnEmbedding(self, rnn_type: str, input_dim: int,output_dim:int , n_neural: int, input_length: int):\n","    print(f'\\n#######################Modelo con capa {rnn_type} ####################################\\n')\n","    print(f'[INFO] Generando el modelo...')\n","    model = Sequential()\n","    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n","    if rnn_type == 'LSTM':\n","          model.add(LSTM(n_neural))\n","    elif rnn_type == 'GRU':\n","        model.add(GRUV2(n_neural))  \n","    elif rnn_type == 'SimpleRNN':\n","        model.add(SimpleRNN(n_neural))\n","    else:\n","        raise ValueError('Invalid RNN type specified. Must be \"LSTM\" or \"GRU\".')\n","    model.add(Dense(1, activation='sigmoid'))\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    print(model.summary())\n","    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","    return model\n","\n","  def evaluate_model(self, model, X, y):\n","    loss, accuracy = model.evaluate(X, y, verbose=0)\n","    y_pred = model.predict(X)\n","    y_pred_binary = np.where(y_pred >= 0.5, 1, 0)\n","    precision = precision_score(y, y_pred_binary)\n","    recall = recall_score(y, y_pred_binary)\n","    f1 = f1_score(y, y_pred_binary)\n","    metrics = {'loss': loss, 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n","    \n","    # Imprimir las métricas de forma legible para el usuario\n","    print(f\"Loss: {metrics['loss']:.4f}\")\n","    print(f\"Accuracy: {metrics['accuracy']*100:.2f}%\")\n","    print(f\"Precision: {metrics['precision']*100:.2f}%\")\n","    print(f\"Recall: {metrics['recall']*100:.2f}%\")\n","    print(f\"F1-score: {metrics['f1']*100:.2f}%\")\n","    return metrics\n","\n","  # def modelCnnW2VEmbedding(self, )\n","    \n","prueba = TrainingCNN(padded_sequences_train,padded_sequences_test, y_train, y_test ,embedding_layer)\n","metrics = prueba.run()"],"metadata":{"id":"uQHKZc5BE3P-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metrics"],"metadata":{"id":"VeWNBTijBbe1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Sequential()\n","model.add(Embedding(input_dim=9208, output_dim=32, input_length=1200))\n","model.add(LSTM(100))\n","model.add(Dense(1, activation='sigmoid'))\n","print(model.summary())\n","model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","model.fit(padded_sequences_train,y_train, validation_split=0.2,\n","          batch_size=64, epochs=1)"],"metadata":{"id":"ErqggW-9CLPz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. unificar metricas en modelos normales en un df\n","2. conclusiones y graficar resultados\n","3. comentar clases \n","4. readme"],"metadata":{"id":"EmpUchAohm7q"}},{"cell_type":"code","source":[],"metadata":{"id":"v3jsQl8UhvRx"},"execution_count":null,"outputs":[]}]}