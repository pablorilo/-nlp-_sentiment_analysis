{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOPpljscm4awO+A8LtqnrCq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["#Extracción de características y entrenamiento de modelos"],"metadata":{"id":"pryjZDvgHBiK"}},{"cell_type":"markdown","source":["###POr un lado, en este notebook vamos a realizar la extracciòn de caracteríticas de dos formas diferentes, mediante CountVectorizer y TfidfVectorizer, posteriormente realizaremos una seleccion de características con SelectKBest(chi2) y realizaremos el entrenamiento de los modelos con los dos set de datos con características diferentes. Es decir vamos a generar dos pipeline diferentes y veremos como se comporta en el entrenamiento.\n","**Pipelines:**\n","\n","TfidfVectorizer -> SelectKBest(chi2) -> LogisticRegression\n","\n","CountVectorizer -> SelectKBest(chi2) -> LogisticRegression\n","\n","TfidfVectorizer -> SelectKBest(chi2) -> GradientBoostingClassifier\n","\n","CountVectorizer -> SelectKBest(chi2) -> GradientBoostingClassifier"],"metadata":{"id":"8C43KaGPHLHZ"}},{"cell_type":"markdown","source":["###Por otro lado vamos a entrenar varios modelos de deep learning. Primeramente entrenaremos 3 modelos con el Embedding realizado en la propia red y probaremos con tres capas diferentes  ['LSTM','GRU','SimpleRNN']"],"metadata":{"id":"MbuWiMEu6FLv"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Qv4NPkKy7hw2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###En la siguiente linea de codigo cargamos los diccionario con los sets de que necesitamos para el entrenamiento. Por un lado dic_training_sets con el conjunto de train y test para entrenar el modelo de regresión logística y boosting y por otro lado el dic_cnn_sets que contiene los datos para realizar la cnn con el embedding en una capa y que tambien contiene la embedding layer generada con word2vec en el notebook preprocesing"],"metadata":{"id":"j12Td5BwIH_m"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1IsEJWVy7XqD"},"outputs":[],"source":["import pickle\n","path = '/content/drive/MyDrive/NLP_practica/datasets/' #path en donde esta almacenado el df\n","\n","# carga el diccionario desde el archivo\n","with open(f'{path}dic_training_sets.pkl', 'rb') as archivo:\n","    dic_training_sets = pickle.load(archivo)\n","    \n","with open(f'{path}dic_cnn_sets.pkl', 'rb') as archivo:\n","    dic_cnn_sets = pickle.load(archivo)\n","    "]},{"cell_type":"code","source":["import codecs\n","import os\n","import pandas as pd\n","import numpy as np\n","import string\n","import random\n","import pickle\n","\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.probability import FreqDist\n","\n","from keras.models import Sequential\n","from keras.layers import Embedding, LSTM, Dense, GRUV2, SimpleRNN\n","\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","from sklearn.model_selection import train_test_split,GridSearchCV # Modelado\n","from sklearn.pipeline import Pipeline # Modelado\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer # Modelado\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import chi2\n","from sklearn.linear_model import LogisticRegression # Reporte\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, precision_recall_curve\n","\n","path = '/content/drive/MyDrive/NLP_practica/datasets/' #path en donde esta almacenado el df"],"metadata":{"id":"DnclTQX2hgGX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Vamos a generar una clase que se encarga de todo el proceso de forma automática. Es decir, podremos indicarle el tipo de extraccion(tfidf o countvectorizer) y el modelo a entrenar(regresion o GradientBoostingClassifier).\n","La clase automaticamente carga los hyperparámetros a validar y sus rangos, es posible realizar modificaciones de dichos rangos pasando dicho hyperparametro como argumento con el nuevo rango a validar al instanciar la clase. Por lo tanto automaticamente genera el pipeline solicitado, hace validación cruzada de parámetros y entrena la mejor combinación, devolviendo los resultados. Durante el proceso ira generando info sobre el paso que está realizando"],"metadata":{"id":"J9QhHZVHIRte"}},{"cell_type":"code","source":["class TextPipelineLogisticAndBoos():\n","  \"\"\"Esta clase genera el pipeline indicado realizando la extracción de características, calcula los mejores hyperparametros, entrena el modelo con dos posibles algoritmos logistic regresion y \n","  GradientBoostingClassifier con dichos parametros y realiza las predicciones.  A lo largo del proceso se iran mostrando en pantalla información del paso que se está realizando asi como algun dato de interes\"\"\"\n","\n","  def __init__(self, method_extraction:str = 'tfidf', model:str= 'regresion',tfidf__ngram_range: list= [(1, 1), (1, 2)], tfidf__min_df: list = [1, 2],\n","               tfidf__max_df:list = [0.9, 0.95],tfidf__max_features:list = [1000,2500], \n","               chi2__k:list = [10, 100], logistic__C:list = [0.1, 1.0],\n","               n_estimators:list = [50, 100], max_depth:list= [3, 5], learning_rate:list = [0.01,0.1]) -> None:\n","    \"\"\"el constructor de la clase crea las variables de clase que contienen el metodo de extracción y el modelo a entrenar. Luego llama a funciones para crear el diccionario de hiperparámetros con los \n","    rangos a validar(establecidos por defecto), si se quieren modificar se pueden pasar como argumento al instanciar la clase ,y el pipeline elgido guardandolo en variables de clase.\n","      :param method_extraction: metodo por defecto tfidf, opcion 2: 'countVectorizer\n","      :param model: modelo por defecto regresion, opcion 2: 'boosting\n","      :params hyper:tfidf__ngram_range: list= [(1, 1), (1, 2)], tfidf__min_df: list = [1, 2],\n","               tfidf__max_df:list = [0.9, 0.95],tfidf__max_features:list = [1000,2500], \n","               chi2__k:list = [10, 100], logistic__C:list = [0.1, 1.0],\n","               n_estimators:list = [50, 100], max_depth:list= [3, 5], learning_rate:list = [0.01,0.1] \"\"\"\n","    self.method_extraction = method_extraction\n","    self.model = model\n","    self.params = self.createDictHyperparams(tfidf__ngram_range= tfidf__ngram_range, tfidf__min_df = tfidf__min_df,\n","               tfidf__max_df= tfidf__max_df,tfidf__max_features= tfidf__max_features, \n","               chi2__k= chi2__k, logistic__C = logistic__C,\n","               n_estimators= n_estimators, max_depth= max_depth, learning_rate= learning_rate)\n","    self.pipe = self.pipelineMethod()\n","    \n","  def run(self,X_train: pd.Series, X_test: pd.Series, y_train: pd.Series, y_test: pd.Series) -> tuple:\n","    \"\"\"este método ejecuta todo el pipeline indicado automaticamente, y va generando mensajes durante todo el proceso\n","      : param X_train: pd.Series  conjunto de train\n","      : param X_test: pd.Series  conjunto de test\n","      : param y_train: pd.Series  etiquetas de train\n","      : param y_test: pd.Series  etiquetas de test\n","      : param method_extraction:str = 'tfidf' parámetro que define el pipeline,  por defecto TfidfVectorizer. se puede pasar por parámetro 'countVectorizer\n","      return : tuple --metrics,train_predict, test_predict\"\"\"\n","    name_model = f'{self.method_extraction}_{self.model}'\n","    pipeline_steps = list(self.pipe.named_steps.keys())\n","    print('\\n#################################################################################\\n')\n","    print(f'Iniciamos el cross-validation del pipeline: {pipeline_steps[0]} ->  {pipeline_steps[1]} -> {pipeline_steps[2]}.')\n","    print('\\n#################################################################################')\n","    print('\\nHiperparámetros a validar en la cross-validation:')\n","    self.printDict(self.params)\n","    self.best_model = self.gridSearch(pipeline = self.pipe, params = self.params)\n","    model = self.fitGridModel(best_model = self.best_model, X_train= X_train, y_train= y_train)\n","    print('\\nObtenemos los valores optimos para los hiperparámtros')\n","    print('<<<<<<<<<<<<<<<<<Best Hyperparamns>>>>>>>>>>>>>>>>>>>')\n","    self.printDict(model.best_params_)\n","    train_predict, test_predict = self.predictModel(model,X_train= X_train,X_test= X_test)\n","    print(f'\\nMétricas obtenidas en el conjunto de train mediante {pipeline_steps[2]}')\n","    metrics_train = self.evaluateModel(y_train,train_predict, 'train')\n","    print(f'\\nMétricas obtenidas en el conjunto de test mediante {pipeline_steps[2]}')\n","    metrics_test = self.evaluateModel(y_test,test_predict, 'test')\n","    metrics={'name_model':name_model}\n","    metrics.update(metrics_train)\n","    metrics.update(metrics_test)\n","    return metrics,train_predict, test_predict\n","\n","    \n","  def createDictHyperparams(self,tfidf__ngram_range: list, tfidf__min_df: list,\n","               tfidf__max_df:list,tfidf__max_features:list, \n","               chi2__k:list, logistic__C:list ,\n","               n_estimators:list, max_depth:list, learning_rate:list):\n","    \"\"\"este metodo genera el diccionario de parametros para el grissearch \"\"\"\n","    print('[INFO] Generando diccionario con hyperparametros a validar...')\n","    if self.method_extraction == 'tfidf':\n","      param_extract = {\n","      'tfidf__ngram_range':  tfidf__ngram_range,\n","      'tfidf__min_df': tfidf__min_df,\n","      'tfidf__max_df': tfidf__max_df,\n","      'tfidf__max_features': tfidf__max_features,\n","      'chi2__k': chi2__k,      \n","                        }\n","    if self.method_extraction == 'countVectorizer':\n","      param_extract = {\n","      'chi2__k': chi2__k\n","                     }\n","\n","    if self.model == 'regresion':\n","      param_model = {\n","      'logistic__C': logistic__C\n","                      }\n","    if self.model == 'boosting':\n","      param_model = {\n","      'boosting__n_estimators': n_estimators,\n","      'boosting__max_depth': max_depth,\n","      'boosting__learning_rate': learning_rate\n","      }            \n","    return {**param_extract,**param_model}\n","\n","  def pipelineMethod(self) -> Pipeline:\n","    \"\"\"método que carga el pipeline dependiendo del método de extraccion y model pasado al instaciar la clase\n","      return : Pipeline\"\"\"\n","    print('[INFO] Generando el pipeline...')\n","    if self.method_extraction == 'tfidf':\n","      if self.model =='regresion':\n","        pipeline = Pipeline([\n","            ('tfidf', TfidfVectorizer(ngram_range=(1, 1), min_df=1, max_df=1.0, max_features = 2500)),\n","            ('chi2', SelectKBest(chi2)),\n","            ('logistic', LogisticRegression(max_iter=1000))\n","        ])\n","      if self.model =='boosting':\n","        pipeline = Pipeline([\n","            ('tfidf', TfidfVectorizer(ngram_range=(1, 1), min_df=1, max_df=1.0, max_features = 2500)),\n","            ('chi2', SelectKBest(chi2)),\n","            ('boosting', GradientBoostingClassifier())\n","        ])\n","      return pipeline\n","    if self.method_extraction == 'countVectorizer':\n","      if self.model =='regresion':\n","        pipeline = Pipeline([\n","            ('countvectorizer',CountVectorizer()),\n","            ('chi2', SelectKBest(chi2)),\n","            ('logistic', LogisticRegression())\n","          ]) \n","      if self.model =='boosting':\n","        pipeline = Pipeline([\n","            ('countvectorizer',CountVectorizer()),\n","            ('chi2', SelectKBest(chi2)),\n","            ('boosting', GradientBoostingClassifier())\n","          ])         \n","    return pipeline    \n","\n","  def gridSearch(self, pipeline: Pipeline, params: dict, cv: int = 5) -> GridSearchCV:\n","    \"\"\"metodo que carga el GridSearchCV\n","     : param pipeline: Pipeline \n","     : param params: dict contien los hyperparámetros y los rangos a validar\n","     : param cv: int por defecto 5. número de validaciones\n","     return : objeto de la clase GridSearchCV\"\"\"\n","    print('\\n[INFO] Creando el gridSeach ...')\n","    grid_search = GridSearchCV(pipeline, params, cv=cv, n_jobs=-1, verbose=1)    \n","    return grid_search\n","\n","  def fitGridModel(self, best_model: GridSearchCV, X_train: pd.Series, y_train: pd.Series) -> GridSearchCV:\n","    \"\"\"con este metedo se realiza el entrenamiento en base al modelo GridSearchCV pasado por parámetro\n","      : param best_model: GridSearchCV     \n","      : param X_train: pd.Series\n","      : param X_train: pd.Series\n","      return GridSearchCV entrenado \"\"\"\n","    print('\\n[INFO] Realizando el entrenamiento ...')\n","    best_model.fit(X_train, y_train)\n","    return best_model\n","\n","  def predictModel(self, model, X_train: pd.Series,X_test: pd.Series) -> tuple:\n","    \"\"\"con este metedo se realiza la prediccion del conjunto de train t test\n","      : param model: GridSearchCV     \n","      : param X_train: pd.Series\n","      : param X_train: pd.Series\n","      return tuple (train_predict, test_predict)\"\"\"\n","    print('\\n[INFO] Realizando las predicciones del conjunto de train ...')\n","    train_predict = model.predict(X_train)\n","    print('[INFO] Realizando las predicciones del conjunto de test ...')\n","    test_predict = model.predict(X_test)\n","    return train_predict, test_predict\n","\n","  @staticmethod\n","  def evaluateModel(y_true: pd.Series, y_pred: pd.Series, set:str) -> dict:\n","    \"\"\"método que calcula e imprime por pantalla diferentes métricas\n","      : param y_true: etiquetas\n","      : param y_pred: predicciones\n","      return: dict con metricas\"\"\"\n","    acc = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","    cm = confusion_matrix(y_true, y_pred)\n","    metrics = {\n","        f'accuracy_{set}': acc,\n","        f'precision_{set}': precision,\n","        f'recall_{set}': recall,\n","        f'f1_{set}' : f1,\n","        f'conf_matx_{set}': cm\n","    }\n","    print('Accuracy: {:.4f}'.format(acc))\n","    print('Precision: {:.4f}'.format(precision))\n","    print('Recall: {:.4f}'.format(recall))\n","    print('F1-Score: {:.4f}'.format(f1))\n","    print('Confusion Matrix:\\n{}'.format(cm))\n","    return metrics\n","\n","  def printDict(self,dic: dict) -> None:\n","    \"\"\"metodo al que le pasas un diccionario e itera en los items para imprimirlos '{key}:    {value}'\n","     : param dic: dict\n","     return: None\"\"\"\n","    for key, value in dic.items():\n","      print(f'{key}:    {value}')\n","\n","\n"],"metadata":{"id":"KeGk40GluQfW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#vamos a guardar en variables los diferentes set que hay en mi_diccionario\n","x_train = dic_training_sets['X_train']\n","y_train = dic_training_sets['y_train']\n","x_test = dic_training_sets['X_test']\n","y_test = dic_training_sets['y_test']"],"metadata":{"id":"Q0jkTRa89Nrx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Ahora vamos a crear las diferentes instancias para el entrenamiento de los diferentes modelos y extractores de características"],"metadata":{"id":"Q-rrIDKXyA6H"}},{"cell_type":"markdown","source":["##1.    TfidfVectorizer -> SelectKBest(chi2) -> LogisticRegression "],"metadata":{"id":"7vJ0PxX4zs-e"}},{"cell_type":"code","source":["#Creamos la instacia a la clase TextPipelineLogistic que se encarga del análisis\n","pipeline_tdidf_reg = TextPipelineLogisticAndBoos(method_extraction='tfidf', model='regresion')"],"metadata":{"id":"jyXlmSHu7l9r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#llamamos al método run de la clase instanciada para que genere el pipeline con method_extraction='tfidf'\n","metrics_tfidf_reg,train_pred_tfidf_reg, test_pred_tfidf_reg = pipeline_tdidf_reg.run(X_train = x_train, X_test = x_test, y_train = y_train, y_test = y_test)"],"metadata":{"id":"FHiaswos-mZ2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##2.    CountVectorizer -> SelectKBest(chi2) -> LogisticRegression"],"metadata":{"id":"kEw5IW-t1RFk"}},{"cell_type":"code","source":["pipeline_countV_reg = TextPipelineLogisticAndBoos(method_extraction='countVectorizer', model='regresion')"],"metadata":{"id":"W1MbinL0028u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#llamamos al método run de la clase instanciada para que genere el pipeline \n","metrics_vec_reg,train_pred_vec_reg, test_pred_vec_reg = pipeline_countV_reg.run(X_train = x_train, X_test = x_test, y_train = y_train, y_test = y_test)"],"metadata":{"id":"G3Ve--nH66zS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##3. TfidfVectorizer -> SelectKBest(chi2) -> GradientBoostingClassifier"],"metadata":{"id":"jLPQe70J1ipg"}},{"cell_type":"code","source":["pipeline_tdidf_boos = TextPipelineLogisticAndBoos(method_extraction='tfidf', model='boosting')"],"metadata":{"id":"3aqDPhRN4DWw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#llamamos al método run de la clase instanciada para que genere el pipeline \n","metrics_tfidf_boos,train_pred_tfidf_boos, test_pred_tfidf_boos = pipeline_tdidf_boos.run(X_train = x_train, X_test = x_test, y_train = y_train, y_test = y_test)"],"metadata":{"id":"nR9Nyo7e1-pY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####4. CountVectorizer -> SelectKBest(chi2) -> GradientBoostingClassifier"],"metadata":{"id":"HTPC2mOT2KQd"}},{"cell_type":"code","source":["pipeline_countV_boos = TextPipelineLogisticAndBoos(method_extraction='countVectorizer', model='boosting')"],"metadata":{"id":"JqYQiEEX2OYs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#llamamos al método run de la clase instanciada para que genere el pipeline \n","metrics_vec_boos,train_pred_vec_boos, test_pred_vec_boos = pipeline_countV_boos.run(X_train = x_train, X_test = x_test, y_train = y_train, y_test = y_test)"],"metadata":{"id":"L_rE7PfK2Tza"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metrics_models_reg_boos = pd.DataFrame(columns=['name_model','accuracy_train', 'precision_train', 'recall_train', 'f1_train',  'accuracy_test', 'precision_test', 'recall_test', 'f1_test'])"],"metadata":{"id":"ZDBlDg33cOj9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metrics_models_reg_boos = metrics_models_reg_boos.append(metrics_vec_reg, ignore_index=True)\n","metrics_models_reg_boos = metrics_models_reg_boos.append(metrics_vec_boos, ignore_index=True)\n","metrics_models_reg_boos = metrics_models_reg_boos.append(metrics_tfidf_reg, ignore_index=True)\n","metrics_models_reg_boos = metrics_models_reg_boos.append(metrics_tfidf_boos, ignore_index=True)"],"metadata":{"id":"IXt128EgZhyQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Especificamos la ruta de la carpeta en donde se guardará, si la carpeta aun no existe la creará\n","path = '/content/drive/MyDrive/NLP_practica/datasets/'\n","metrics_models_reg_boos.to_csv(f'{path}metrics_models_reg_boos.csv', index=False)\n","\n","metrics_models_reg_boos"],"metadata":{"id":"xJUvYkjZZtuA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-2XUnH0VfyfR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Modelo con Deep Learning"],"metadata":{"id":"610FVtDFTV2h"}},{"cell_type":"markdown","source":["###Vamos a evaluar 3 modelos con el embedding realizado con la funcion de keras Embedding, probando con 3 capas diferentes 'LSTM' , 'GRU' y 'SimpleRNN'\n","Para ello vamos a crear una clase que realiza de forma automatica el modelado de los tres sistemas, devolviéndonos las diferntens métricas en pantalla y a su vez en un df"],"metadata":{"id":"fOMjtWj8Taw2"}},{"cell_type":"code","source":["padded_sequences_train = dic_cnn_sets['padded_sequences_train']\n","padded_sequences_test = dic_cnn_sets['padded_sequences_test']\n","y_train = dic_cnn_sets['y_train']\n","y_test = dic_cnn_sets['y_test']\n","embedding_layer = dic_cnn_sets['embedding_layer']"],"metadata":{"id":"S7Vk-d7Rcmsw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TrainingCNN:\n","  \"\"\"esta clase realiza el entrenamiento con CNN con diferentes capas\"\"\"\n","  def __init__(self,padded_sequences_train,padded_sequences_test, y_train, y_test ,embedding_layer) -> None:\n","    \"\"\"el constructor genera las varibles de clase que almacena los pad_sets los y´s y el embedding_layer\n","      :param padded_sequences_train: conjunto de train con padding\n","      :param padded_sequences_test: conjunto de test con padding\n","      :param y_train: etiquetas del train\n","      :param y_test: etiquetas del test\"\"\"\n","    self.padded_sequences_train= padded_sequences_train\n","    self.padded_sequences_test= padded_sequences_test\n","    self.y_train = y_train\n","    self.y_test = y_test\n","    self.embedding_layer = embedding_layer\n","    \n","  def run(self, input_dim: int=10882, output_dim: int=100, input_length: int=1200,n_neural: int = 100) -> pd.DataFrame:\n","    \"\"\"metodo que ejecuta la creacion de los modelos, sus entrenamientos y almacena las metricas de los mismos en un df\n","      : param input_dim: int dimension de entrada 10882 \n","      : param output_dim: int dimension de salid 100\n","      : param  input_length: int tamaño máximo\n","      : param n_neural: numero de neuronas\n","      return: pd.DataFrame con las metricas de los 3 modelos\"\"\"\n","    #Definimos una variable que contiene los metodos de la capa profunda\n","    type_layers = ['LSTM','GRU','SimpleRNN']\n","    #Generamos un df para cada tipo de cnn entrenada(embedding en capa, Word2Vec) donde iremos almacenando las metricas obtenidas en los entrenamientos y predicciones\n","    metrics_cnn_df = pd.DataFrame(columns=['layer', 'loss_train', 'accuracy_train', 'precision_train', 'recall_train', 'f1_train', 'loss_test', 'accuracy_test', 'precision_test', 'recall_test', 'f1_test'])\n","    metrics_cnn_custom_df = pd.DataFrame(columns=['layer', 'loss_train', 'accuracy_train', 'precision_train', 'recall_train', 'f1_train', 'loss_test', 'accuracy_test', 'precision_test', 'recall_test', 'f1_test'])\n","    \n","    #Vamos a ejecutar los entrenamientos de forma automática con dos bucles, uno para cada modelo, que iran realando entrenamiento con las 3 type_layers\n","    #Primero el modelo con el embedding realizado en la capa de la CNN\n","    \n","    for layer in type_layers:\n","      print('\\n#################################################################################')\n","      print(f'\\n            Generando modelo con Embedding en capa profunda')\n","      self.model_cnn_embedding = self.modelCnnEmbedding(rnn_type = layer, input_dim=input_dim, output_dim=output_dim, input_length=input_length,n_neural= n_neural)\n","      print(f'\\n[INFO]Entrenando modelo CNN con capa {layer}')\n","      self.model_cnn_embedding.fit(self.padded_sequences_train,self.y_train, validation_split=0.2,batch_size=64, epochs=1)\n","      print('\\nGeneramos las métricas de train:')\n","      metrics_train = self.evaluate_model(self.model_cnn_embedding,self.padded_sequences_train,self.y_train)\n","      print('\\nGeneramos las métricas de test:')\n","      metrics_test = self.evaluate_model(self.model_cnn_embedding,self.padded_sequences_test,self.y_test)\n","      metrics_dict = {'layer': layer,\n","                        'loss_train': metrics_train['loss'],\n","                        'accuracy_train': metrics_train['accuracy'],\n","                        'precision_train': metrics_train['precision'],\n","                        'recall_train': metrics_train['recall'],\n","                        'f1_train': metrics_train['f1'],\n","                        'loss_test': metrics_test['loss'],\n","                        'accuracy_test': metrics_test['accuracy'],\n","                        'precision_test': metrics_test['precision'],\n","                        'recall_test': metrics_test['recall'],\n","                        'f1_test':metrics_test['f1']\n","                       }\n","        \n","      metrics_cnn_df = metrics_cnn_df.append(metrics_dict, ignore_index=True)\n","\n","    #realizamos el mismo proceso con la capa de embedding realizada con W2V\n","    for layer in type_layers:\n","      print('\\n#################################################################################')\n","      print(f'\\n            Generando modelo con Embedding customizado W2V')\n","      self.model_cnn_embedding = self.modelCnnW2VEmbedding(rnn_type = layer, n_neural = n_neural)\n","      print(f'\\n[INFO]Entrenando modelo CNN con capa {layer}')\n","      self.model_cnn_embedding.fit(self.padded_sequences_train,self.y_train, validation_split=0.2,batch_size=64, epochs=1)\n","      print('\\nGeneramos las métricas de train:')\n","      metrics_train = self.evaluate_model(self.model_cnn_embedding,self.padded_sequences_train,self.y_train)\n","      print('\\nGeneramos las métricas de test:')\n","      metrics_test = self.evaluate_model(self.model_cnn_embedding,self.padded_sequences_test,self.y_test)\n","      metrics_dict = {'layer': layer,\n","                        'loss_train': metrics_train['loss'],\n","                        'accuracy_train': metrics_train['accuracy'],\n","                        'precision_train': metrics_train['precision'],\n","                        'recall_train': metrics_train['recall'],\n","                        'f1_train': metrics_train['f1'],\n","                        'loss_test': metrics_test['loss'],\n","                        'accuracy_test': metrics_test['accuracy'],\n","                        'precision_test': metrics_test['precision'],\n","                        'recall_test': metrics_test['recall'],\n","                        'f1_test':metrics_test['f1']\n","                       }\n","        \n","      metrics_cnn_custom_df = metrics_cnn_custom_df.append(metrics_dict, ignore_index=True)\n","    \n","    return  metrics_cnn_df, metrics_cnn_custom_df\n","\n","  def modelCnnEmbedding(self, rnn_type: str, input_dim: int,output_dim:int , n_neural: int, input_length: int):\n","    \"\"\"este metodo generea y compila un modelo cnn con el embedding automatico de keras Embedding\n","     :param rnn_type: tipo de capa. Opciones ['LSTM','GRU','SimpleRNN']\n","     return: model\"\"\"\n","    print(f'\\n####################### Modelo con capa {rnn_type} ####################################\\n')\n","    print(f'[INFO] Generando el modelo...')\n","    model = Sequential()\n","    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n","    if rnn_type == 'LSTM':\n","          model.add(LSTM(n_neural))\n","    elif rnn_type == 'GRU':\n","        model.add(GRUV2(n_neural))  \n","    elif rnn_type == 'SimpleRNN':\n","        model.add(SimpleRNN(n_neural))\n","    else:\n","        raise ValueError('Invalid RNN type specified. Must be \"LSTM\" or \"GRU\".')\n","    model.add(Dense(1, activation='sigmoid'))\n","    print(model.summary())\n","    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","    return model\n","\n","  def evaluate_model(self, model, X, y) -> dict:\n","    \"\"\"calcula, almacena y muestra diferentes métricas del modelo\n","      : param model: modelo a analizar\n","      : param X: set de datos \n","      : param y: etiquetas de x\n","      return dict: con todas las metricas\"\"\"\n","    loss, accuracy = model.evaluate(X, y, verbose=0)\n","    y_pred = model.predict(X)\n","    y_pred_binary = np.where(y_pred >= 0.5, 1, 0)\n","    precision = precision_score(y, y_pred_binary)\n","    recall = recall_score(y, y_pred_binary)\n","    f1 = f1_score(y, y_pred_binary)\n","    metrics = {'loss': loss, 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n","    \n","    # Imprimimos las métricas de forma legible para el usuario\n","    print(f\"Loss: {metrics['loss']:.4f}\")\n","    print(f\"Accuracy: {metrics['accuracy']*100:.2f}%\")\n","    print(f\"Precision: {metrics['precision']*100:.2f}%\")\n","    print(f\"Recall: {metrics['recall']*100:.2f}%\")\n","    print(f\"F1-score: {metrics['f1']*100:.2f}%\")\n","    return metrics\n","\n","  def modelCnnW2VEmbedding(self,rnn_type: str,n_neural: int ):\n","    \"\"\"Realiza el entrenamiento con la capa de embedding que se le pasa por parámetro\n","      : param rnn_type: metodo a aplicar en la capa profunda de la CNN\n","      : param n_neural: numero de neuronas a usar\n","      return \"\"\"\n","    print(f'\\n####################### Modelo con capa {rnn_type} ####################################\\n')\n","    print(f'[INFO] Generando el modelo...')\n","    model_custom = Sequential()\n","    #pasamos directamente la capa que hemos generado\n","    model_custom.add(self.embedding_layer)\n","    if rnn_type == 'LSTM':\n","      model_custom.add(LSTM(n_neural,dropout=0.2, recurrent_dropout=0.2))\n","    elif rnn_type == 'GRU':\n","      model_custom.add(GRUV2(n_neural,dropout=0.2, recurrent_dropout=0.2))  \n","    elif rnn_type == 'SimpleRNN':\n","      model_custom.add(SimpleRNN(n_neural))\n","    else:\n","      raise ValueError('Invalid RNN type specified. Must be \"LSTM\" or \"GRU\".')\n","    model_custom.add(Dense(1, activation=\"sigmoid\"))\n","    print(model_custom.summary())\n","    model_custom.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","    return model_custom\n","\n","    \n"],"metadata":{"id":"uQHKZc5BE3P-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Creamos la instancia a la clase\n","cnn_training = TrainingCNN(padded_sequences_train,padded_sequences_test, y_train, y_test ,embedding_layer)\n","#Llamamos a la funcion run para desarrollar los entrenamientos y generar un df con las metricas de los 3\n","metrics_cnn, metrics_cnn_custom = cnn_training.run()"],"metadata":{"id":"eLt4Its_UXdc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Realizamos el guardado del df para su uso en el notebook de conclusiones\n","metrics_cnn.to_csv(f'{path}metrics_cnn.csv', index=False)\n","metrics_cnn"],"metadata":{"id":"2yFSYvkpiTs6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Ku9Wk_1r8UuG"},"execution_count":null,"outputs":[]}]}